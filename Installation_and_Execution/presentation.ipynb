{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"presentation.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5d3f52ea"},"source":["# <font color=blue><center>Retail Sales Analysis</center></font>\n","## Agenda\n","\n","### Why?\n","Walmart is an American retail corporation that operates a chain of hypermarkets, discount department stores, and grocery stores. In this usecase, we will answer the following question:\n","\n","- Which store has minimum and maximum sales?\n","- Which store has maximum standard deviation?\n","- Which store/s has good quarterly growth rate in Q3’2012?\n","- Find out holidays which have higher sales than the mean sales in non-holiday season for all stores together.\n","\n","### Architecture\n","- Overview of data flow\n","- Tech Stack\n","\n","### Environment Setup\n","- AWS EC2 instance and security group creation\n","- Docker installation and running\n","- Usage of docker-composer and starting all the tools\n","- How to access tools in local machine\n","\n","### Deep dive - HDFS\n","- Introduction (Overview & Why is it needed?)\n","- Terminology\n","- Architecture\n","- File processing\n","- Various file formats\n","- Frequently used commands\n","\n","### Deep dive - Sqoop\n","- Introduction\n","- Architecture\n","- Import\n","- Export\n","- Job\n","- Useful tools/commands\n","\n","### Deep dive - Hive\n","- Introduction\n","- Basic Commands\n","- Functions\n","- Partitioning\n","- Bucketing\n","- Joins and Views\n","- Various file formats\n","- Custom input formatter\n","- SCD implementation\n","\n","### MySQL Set up\n","- Download Dataset\n","- Table creation\n","\n","### Extraction\n","- Import SQL data using Sqoop\n","- Job creation\n","\n","### Transformation and Load\n","- Table creation\n","- Queries to answer questions\n","\n","### Project Overview"],"id":"5d3f52ea"},{"cell_type":"markdown","metadata":{"id":"601a4f18"},"source":["## <font color=blue>Architecture</font>\n","### Overview of data flow\n","#### Data Flow Architecture\n","![alt text](images/Retail_Analysis_Architecture.png)\n","### Tech Stack\n","* AWS EC2\n","* Docker\n","* Jupyter Lab\n","* MySQL\n","* Sqoop\n","* HDFS\n","* Hive"],"id":"601a4f18"},{"cell_type":"markdown","metadata":{"id":"b0e00d53"},"source":["## <font color=blue>Environment Setup</font>\n","### AWS EC2 instance and security group creation\n","- t2.xlarge instance\n","- 32GB of storage recommended\n","- Allow ports 4000 - 38888\n","- Connect to ec2 via ssh\n"," <code>ssh -i \"D:\\path\\to\\private\\key.pem\" user@Public_DNS</code>\n"," <br/>Example:<code>ssh -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\" ec2-user@ec2-54-203-235-65.us-west-2.compute.amazonaws.com</code><br/>\n","- Port forwarding \n"," <code>ssh -i \"D:\\path\\to\\private\\key.pem\" user@Public_DNS</code>\n"," <br/>Example:<code>ssh -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\" ec2-user@ec2-34-208-254-29.us-west-2.compute.amazonaws.com -L 2081:localhost:2041 -L 4888:localhost:4888 -L 2080:localhost:2080 -L 8050:localhost:8050 -L 4141:localhost:4141</code><br/>\n","- Copy from local to ec2\n","  <code>scp -r -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\"</code>\n","  <br/>Example:<code>scp -r -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\" D:\\Users\\pyerravelly\\Downloads\\spark-standalone-cluster-on-docker-master\\build\\docker\\docker-exp ec2-user@ec2-34-208-254-29.us-west-2.compute.amazonaws.com:/home/ec2-user/docker_exp\n","</code>\n","\n","### Docker installation and running\n","    \n","### Usage of docker-composer and starting all the tools\n","\n","- Commands to install Docker\n","\n","<code>sudo yum update -y</code>\n","<code><br/>sudo yum install docker</code>\n","<code><br/>sudo curl -L \"https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose</code>\n","<code><br/>sudo chmod +x /usr/local/bin/docker-compose</code>\n","<code><br/>sudo gpasswd -a $USER docker</code>\n","<code><br/>newgrp docker</code>\n","<br/>Start Docker: <code>sudo systemctl start docker</code>\n","<br/>Stop Docker: <code>sudo systemctl stop docker</code>\n","\n","- How to access tools in local machine <br/>\n","    List Docker containers running: <code>docker ps</code><br/>\n","    CLI access in Docker container: <code>docker exec -i -t kafka bash</code><br/>\n","    NiFi at: http://localhost:2080/nifi/ <br/>\n","    Jupyter Lab at: http://localhost:4888/lab? <br/>\n","    HDFS at: http://localhost:50070/\n","    Dash Application at: http://localhost:8050/ (this will be available when executed log_visualizer.ipynb)"],"id":"b0e00d53"},{"cell_type":"markdown","metadata":{"id":"846625c5"},"source":["## <font color=blue> Deep dive - HDFS</font>"],"id":"846625c5"},{"cell_type":"markdown","metadata":{"id":"457c38a1"},"source":["### Introduction (Overview & Why is it needed?)\n","#### Function of File System\n","#### Various File Systems\n","#### How HDFS is different?\n","![alt text](images/hdfs_ov.PNG)\n","\n","###  Terminology\n","- Cluster\n","- NameNode\n","- DataNode\n","- Mapper\n","- Reducer\n","- JobTracker\n","- TaskTracker\n","- Block\n","- Secondary NameNode\n","- Safemode\n","\n","### Key Points\n","- Only one Data Node\n","- Node are computers\n","- Number of Blocks depends on File size\n","- Number of blocks = mappers\n","- Number of Reducers = output\n","\n","###  Architecture\n","![alt text](images/hdfs_arhitecture.png)\n","\n","###  File processing\n","![alt text](images/file_processing_steps.png)\n","\n","###  Various file formats\n","- Text/CSV Files\n","- JSON Records\n","- Avro Files\n","- Sequence Files\n","- RC Files\n","- ORC Files\n","- Parquet Files\n","\n","###  Frequently used commands\n","\n","<code>docker exec -it ra_namenode bash</code></br>\n","<code>hdfs dfs -ls /user/hive/warehouse/raw_sales/part-m-00000</code></br>\n","<code>hdfs dfs -cat /user/hive/warehouse/raw_sales/part-m-00000</code></br>\n","<code>hdfs dfs -cat /user/hive/warehouse/raw_sales/part-m-00000|head -10</code></br>\n","<code>hdfs dfs -touch /demo</code></br>\n","<code>hdfs dfs -chmod 755 /demo</code></br>\n","<code>hdfs dfs -ls /demo</code></br>\n","<code>echo \"example\">local_demo</code></br>\n","<code>hdfs dfs -copyFromLocal local_demo /local_demo</code></br>\n","<code>hdfs dfs -copyToLocal /local_demo local_demo2</code></br>\n","<code>hdfs dfs -count /local_demo</code></br>\n","<code>hdfs dfs -cp -f /local_demo /local_demo4</code></br>\n","<code>hdfs dfs -df -h</code></br>\n","<code>hdfs dfs -du -h /user</code></br>\n","<code>hdfs dfs -help</code></br>\n","<code>hdfs dfs -mkdir /user/tmp</code></br>\n","<code>hdfs dfs -rmdir /user/tmp</code></br>"],"id":"457c38a1"},{"cell_type":"markdown","metadata":{"id":"7ff67606"},"source":["## <font color=blue>Deep dive - Sqoop</font>"],"id":"7ff67606"},{"cell_type":"markdown","metadata":{"id":"4562882d"},"source":["### Introduction\n","- Usage\n","- Features\n","\n","### Architecture\n","- Basic Architecture\n","\n","![alt text](images/sqoop_basic_architecture.png)\n","\n","- Architecture with Task\n","\n","![alt text](images/Sqoop_Architecture.png)\n","\n","#### Login to Sqoop using <code>docker exec -i -t ra_sqoop bash</code>\n","\n","### Import\n","- Basic\n","<code>\n","sqoop import \\\n","  --connect jdbc:mysql://mysql:3306/demo \\\n","  --username root \\\n","  --password example \\\n","  --table employee \\\n","  --m 1\n"," </code>\n","- Target directory\n","<code>\n","    sqoop import \\\n"," --connect jdbc:mysql://mysql:3306/demo \\\n"," --username root \\\n"," --password example \\\n"," --table employee \\\n"," --m 1 \\\n"," --target-dir /empdetails\n","</code>\n","- Based on a condition\n","<code>\n","sqoop import \\\n"," --connect jdbc:mysql://mysql:3306/demo \\\n"," --username root \\\n"," --password example \\\n"," --table employee \\\n"," --m 1 \\\n"," --where \"id =4\" \\\n"," --target-dir /empdetails1\n","</code>\n","- Incremental\n","<code>\n","    sqoop import \\\n"," --connect jdbc:mysql://mysql:3306/demo \\\n"," --username root \\\n"," --password example \\\n"," --table employee \\\n"," --m 1 \\\n"," --incremental append \\\n"," --check-column id \\\n"," --last-value 10\n","</code>\n","- All tables at once\n","<code>\n","    sqoop import-all-tables \\\n"," --connect jdbc:mysql://mysql:3306/demo \\\n"," --username root \\\n"," --password example \\\n"," --autoreset-to-one-mapper\n","</code>\n","\n","\n","### Export\n","- Command\n","<code>\n"," sqoop export \\\n"," --connect jdbc:mysql://mysql:3306/demo \\\n"," --username root \\\n"," --password example \\\n"," --table employee_exp \\\n"," --export-dir employee\n","</code>\n","\n","### Job\n","<code>docker cp java-json-schema.jar ra_sqoop:/opt/sqoop-1.4.7.bin__hadoop-2.6.0/lib/java-json-schema.jar</code>\n","- Command\n","<code>\n"," sqoop job –-create firstjob \\\n"," -- import \\\n"," --connect jdbc:mysql://mysql:3306/demo \\\n"," --username root \\\n"," --password example \\\n"," --table employee \\\n"," --m 1 \\\n"," --target-dir /empdetails_job\n","</code>\n","- Delete\n","<code>\n","    sqoop job --delete firstjob\n","</code>\n","- List\n","<code>\n","    sqoop job --list\n","</code>\n","- Execute\n","<code>\n","    sqoop job --exec myjob\n","</code>\n","\n","### Useful tools/commands\n","- Get List DBs\n","<code>\n"," sqoop list-databases \\\n"," --connect jdbc:mysql://mysql:3306/demo \\\n"," --username root\n","</code>\n","\n","- Get List Tables\n","<code>\n","    sqoop list-tables \\\n"," --connect jdbc:mysql://mysql:3306/demo \\\n"," --username root\n","</code>\n","\n","- SQL Queries\n","<code>\n","    sqoop eval \\\n"," --connect jdbc:mysql://mysql:3306/demo \\\n"," --username root \\\n"," --query \"SELECT * FROM employee LIMIT 3\"\n","</code>"],"id":"4562882d"},{"cell_type":"markdown","metadata":{"id":"9484c5ce"},"source":["## <font color=blue>Deep dive - Hive</font>"],"id":"9484c5ce"},{"cell_type":"markdown","metadata":{"id":"478000e4"},"source":["### Introduction\n","- SQL like querying tool\n","- Structured data\n","- Batch processing\n","- Layer between MR and HDFS\n","- Various file formats\n","- NOT a database\n","- More of OLAP but not suitable for OLTP\n","- Suport for UPSERT in latest versions\n","\n","### Architecture\n","![alt text](images/hive_architecture.png)\n","\n","### Basic Commands\n","<code> docker cp table1.txt ra_hive-server:table1.txt <br/> set hive.cli.print.header=true;<br/>set hive.resultset.use.unique.column.names=false;</code>\n","\n","- Database creation\n","\n","<code> create database if not exists db [comment 'Demo database'] [with dbproperties('creator'='Pavan Kumar Yerravelly')];</br></code>\n","<code> describe database [extended] db; </br></code>\n","<code> show databases; </br></code>\n","<code> use db; </br></code>\n","\n","- Table creation\n","  - Managed Table\n","  <code> create table if not exists table1(col1 int,col2 array&lt;string&gt;,col3 string,col4 int)row format delimited fields terminated by',' collection items terminated by':' lines terminated by'\\n' stored as textfile location '/data/table1';<br/>load data local inpath '/table1.txt' into table table1;<br/></code>\n","  - External Table\n","  <code>create external table if not exists table2(col1 int,col2 array&lt;string&gt;,col3 string,col4 int)row format delimited fields terminated by',' collection items terminated by':' lines terminated by'\\n' stored as textfile location'/data/table2';<br/>hdfs dfs -mkdir /data/table2/<br/>hdfs dfs -copyFromLocal /table1.txt /data/table2/</code>\n","\n","- Insert\n","  - Basic\n","<code> create table table3 like table1;<br/>insert into table3 select * from table1;<br/>insert overwrite table3 select * from table1;<br/>create table table3 as select * from table1;</code>\n","  - Multi\n","<code>create table below like table1;<br/>create table above like table1;<br/> from table2 insert into below select *  where col1&lt;500 insert into above select * where col1&gt;=500; </code> \n","\n","- Alter Table\n","<code><br/> alter table below add columns (col5 int,col6 string);<br/> alter table below change column col1 id int;<br/>desc formatted below;<br/>alter table below set tblproperties('auto.perge'='true')</code>\n","      \n","### Functions\n","- Date <code> https://cwiki.apache.org/confluence/display/hive/languagemanual+udf#LanguageManualUDF-DateFunctions </code>\n","- Mathematical <code> https://cwiki.apache.org/confluence/display/hive/languagemanual+udf#LanguageManualUDF-MathematicalFunctions </code>\n","- String <code> https://cwiki.apache.org/confluence/display/hive/languagemanual+udf#LanguageManualUDF-StringFunctions </code>\n","- Conditional <code> select *,if(col1=499, 'foo','bar') as derived from table1; </code>\n","<code> select case when col1=499 then 'foo' when col1=500 then 'bar' else col1 end as derived from table1; </code>\n","- Explode and Lateral View\n","  - Data<br/>\n","      <code>select explode(col2) from table1;<br/>select col1, exp_col from table1 lateral view explode(col2) exp as exp_col;</code>\n","- Window<br/>\n","<code>create table if not exists table4(col1 string,col2 int) row format delimited fields terminated by',' lines terminated by'\\n'stored as textfile;<br/><br/>insert into table4 values('John',1500),('Albert',1500),('Mark',1000),('Frank',1150),('Loopa',1100),('Lui',1300),('John',1300),('John',900),('Lesa',1500),('Lesa',900),('Pars',800),('leo',700),('leo',1500),('lock',650),('Bhut',800),('Lio',500);<br/><br/>select col1,col2,rank() over(order by col2 desc) as ranking from table4;<br/><br/>select r1.col1,r1.col2,r1.ranking from (select col1,col2,rank() over(order by col2 desc) as ranking from table4) as r1 where r1.ranking&lt2;<br/><br/>select col1,col2,dense_rank() over(order by col2 desc) as ranking from table4;<br/><br/>select r1.col1,r1.col2,r1.ranking from (select col1,col2,dense_rank() over(order by col2 desc) as ranking from table4) as r1 where r1.ranking&lt2;<br/><br/>select col1,col2,row_number() over(order by col2 desc) as ranking from table4;<br/><br/>select r1.col1,r1.col2,r1.ranking from (select col1,col2,row_number() over(order by col2 desc) as ranking from table4) as r1 where r1.ranking &le;2;<br/><br/>select r1.col1,r1.col2,r1.ranking from (select col1,col2,row_number() over(partition by col1 order by col2 desc) as ranking from table4) as r1 where r1.ranking &le;2;</code>\n","      \n","### Partitioning\n","<code>docker cp departments.txt ra_hive-server:departments.txt<br/>create table if not exists department (col1 int,col2 string,col3 string,col4 int) row format delimited fields terminated by',' lines terminated by'\\n'stored as textfile;<br/>load data local inpath'/departments.txt'into table department;</code>\n","\n","- Static\n","<code>create table if not exists part_department (departmentno int,empname string,sal int) partitioned by (departmentname string) row format delimited fields terminated by',' lines terminated by'\\n'stored as textfile;<br/>insert into table part_department partition (departmentname = 'HR') select col1,col3,col4 from department where col2 = 'HR';<br/>load data local inpath'/departments.txt'into table part_department partition( departmentname ='XZ');</code>\n","\n","- Dynamic\n","<code>create table if not exists part_department1 (departmentno int,empname string,sal int) partitioned by (departmentname string) row format delimited fields terminated by',' lines terminated by'\\n'stored as textfile;<br/>set hive.exec.dynamic.partition=true;<br/>set hive.exec.dynamic.partition.mode=nonstrict;<br/>insert into table part_department1 partition (departmentname) select col1,col3,col4,col2 from department; </code>\n","\n","### Bucketing\n","<code>docker cp dept_location.txt ra_hive-server:dept_location.txt<br/>create table if not exists department_with_location (departmentno int,departmentname string,empname string,sal int,location string) row format delimited fields terminated by',' lines terminated by'\\n'stored as textfile;<br/>load data local inpath'/dept_location.txt'into table department_with_location;<br/>set hive.enforce.bucketing=true;<br/>create table if not exists clustered_department (departmentno int,empname string,sal int,location string) partitioned by (departmentname string) clustered by(location) into 4 buckets  row format delimited fields terminated by',' lines terminated by'\\n'stored as textfile;<br/>insert into clustered_department partition (departmentname) select departmentno,empname,sal,location,departmentname from department_with_location;</code>\n","### Joins and Views\n","- Joins\n","<code><br/>select tab1.col1,tab2.col2,tab3.col4 from tab1 join tab2 on (tab1.col4=tab2.col5) join tab3 on (tab2.col6==tab3.col1)<br/>select /*+ STREAMTABLE (tab1) */ tab1.col1,tab2.col2 from tab1 join tab2 on (tab1.col4=tab2.col5) join tab3 on (tab2.col6==tab3.col1);</code>\n"," - Map Joins\n","<code>Hint: select /*+ MAPJOIN (tab1) */ tab1.col1,tab2.col2 from tab1 join tab2 on (tab1.col4=tab2.col5);<br/>config: set hive.auto.convert.join=true;<br/>set hive.mapjoin.smalltable.filesize;</code>\n","\n","- Views\n","<code><br/>create view emp_view1 as select * from emp_tab;<br/>create view emp_view2 as select col1,col2 from emp_tab;<br/>create view if not exists emp_view3 as select col1 as id,col2 as name from emp_tab;<br/>create view emp_view4 as select emp_tab.col1,emp_tab.col2,dept_tab.col3 from emp_tab join dept_tab on (emp_tab.col6 = dept_tab.col1);<br/>alter view emp_view1 as select col1 from emp_tab;<br/>alter view emp_view1 rename to emp_view_new;<br/>drop view emp_view2;<br/></code>\n","\n","### Various file formats\n","- TSV/CSV:\n"," - Read/Write: Reads slow\n"," - Compression: No bblock compression\n"," - Split: Only on new line\n"," - Schema Evolution: Limited\n","- Sequence:\n"," - Read/Write: faster writes that text files\n"," - Compression: Block level\n"," - Split: Yes\n"," - Schema Evolution: Same as text files\n","- Avro:\n"," - Read/Write: Average\n"," - Compression: Block level\n"," - Split: Yes\n"," - Schema Evolution: Yes\n","- RC:\n"," - Read/Write: Faster reads\n"," - Compression: High\n"," - Split: Yes\n"," - Schema Evolution: No\n","- ORC:\n"," - Read/Write: Faster\n"," - Compression: Yes\n"," - Split: Yes, at stripe level\n"," - Schema Evolution: No\n","- Parquet:\n"," - Read/Write: faster reads\n"," - Compression: Yes, with Sanppy\n"," - Split: Limited\n"," - Schema Evolution: Limited\n"," \n","- Which format to choose?\n"," - Schema evolution - Avro\n"," - Dumping data from Filesystem - Text\n"," - Reading - ORC/Parquret\n"," - Intermediate Files - Sequence\n","\n","### SCD Type-1 implementation\n","<code>\n","CREATE TABLE merge_src\n","(\n","ID INT,\n","FirstName VARCHAR(100),\n","LastName VARCHAR(100)\n",");<br/>INSERT INTO merge_src VALUES (1, 'aaaa', 'bbbb'),(2, 'cccc', 'dddd'),(3, 'eeee', 'ffff'),(4, 'gggg', 'hhhh'),(5, 'iiii', 'jjjj');<br/>create table merge_tgt\n","(\n","id int,\n","firstname varchar(100),\n","lastname varchar(100)\n",")\n",";<br/>\n","drop table merge_staging;<br/>create temporary table merge_staging as \n","SELECT A.id        AS ID, \n","       A.firstname AS FirstName, \n","       CASE \n","         WHEN B.id IS NOT NULL THEN B.lastname \n","         ELSE A.lastname \n","       end         AS LastName \n","FROM   merge_tgt AS A \n","       LEFT OUTER JOIN merge_src AS B \n","                    ON A.id = B.id; <br/>INSERT INTO merge_staging \n","SELECT B.id        AS ID, \n","       B.firstname AS FirstName, \n","       B.lastname  AS LastName \n","FROM   merge_src AS B \n","       LEFT OUTER JOIN merge_staging AS A \n","                    ON A.id = B.id \n","WHERE  A.id IS NULL;<br/>insert overwrite table merge_tgt select * from merge_staging;<br/>--Second run<br/>INSERT overwrite table merge_src VALUES (2, 'cccc', 'kkkk'),(3, 'eeee', 'llll'),(7, 'xxxx', 'yyyy');\n","    </code>"],"id":"478000e4"},{"cell_type":"markdown","metadata":{"id":"6e6bb23c"},"source":["## <font color=blue>Extraction</font>"],"id":"6e6bb23c"},{"cell_type":"markdown","metadata":{"id":"d79f701d"},"source":["### MySQL Set up\n","\n","- Download Dataset\n","<code>docker cp Walmart_Store_sales.csv ra_mysql:Walmart_Store_sales.csv </code>\n","- Table creation\n","<code>docker exec -it ra_mysql bash<br/>mysql -p (type example when passsword prompted)<br/>SET GLOBAL local_infile=1;<br/>mysql --local-infile=1 -p<br/>CREATE DATABASE if not exists retail;<br/>use retail;<br/>CREATE TABLE if not exists walmart_sales (Store VARCHAR(255),Date Date,Weekly_Sales VARCHAR(255),Holiday_Flag VARCHAR(255),Temperature VARCHAR(255),Fuel_Price VARCHAR(255),CPI VARCHAR(255),Unemployment VARCHAR(255));<br/>show tables;<br/>LOAD DATA LOCAL INFILE '/Walmart_Store_sales.csv' INTO TABLE walmart_sales FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\\\n' IGNORE 1 ROWS (Store,@Date,Weekly_Sales,Holiday_Flag,Temperature,Fuel_Price,CPI,Unemployment) SET Date=STR_TO_DATE( @Date, '%d-%m-%Y' );\n","</code>\n","\n","### Sqoop Import\n","\n","- Import SQL data using Sqoop\n","<code>docker exec -i -t sqoop bash<br/>sqoop import \\\\\n"," --connect jdbc:mysql://mysql:3306/retail \\\\\n"," --username root \\\\\n"," --password example \\\\\n"," --table walmart_sales \\\\\n"," --fields-terminated-by \",\" \\\\\n"," --hive-import \\\\\n"," --hive-table raw_sales \\\\\n"," --hive-overwrite \\\\\n"," --m 1 \\\\\n"," --incremental append \\\\\n"," --check-column Date \\\\\n"," --last-value '1900-01-01'\n","</code>\n","\n","- Job creation\n","<code>sqoop job --create retail_job \\\\\n"," -- import \\\\\n"," --connect jdbc:mysql://mysql:3306/retail \\\\\n"," --username root \\\\\n"," --password example \\\\\n"," --table walmart_sales \\\\\n"," --fields-terminated-by \",\" \\\\\n"," --hive-import \\\\\n"," --hive-table raw_sales \\\\\n"," --hive-overwrite \\\\\n"," --m 1 \\\\\n"," --incremental append \\\\\n"," --check-column Date \\\\\n"," --last-value '1900-01-01'<br/>sqoop job --exec retail_job\n","</code>\n","\n","- Incremental Load\n","<code>insert into walmart_sales values(45,'2012-11-03','760281.43','0','58.85','3.882','192.3088989','8.667');</code>"],"id":"d79f701d"},{"cell_type":"markdown","metadata":{"id":"86ec60fe"},"source":["## <font color=blue>Transformation and Load</font>\n","\n","\n","- Table creation\n","<code>\n","beeline\n","!connect jdbc:hive2://127.0.0.1:10000 scott tiger\n","create database if not exists retail;\n","use retail;\n","create table walmart_sales as \n","select \n","    cast(Store as int),\n","    cast(to_date(from_unixtime(unix_timestamp(`Date`, 'yyyy-MM-dd'))) as date) as date_of_entry,\n","    cast(Weekly_Sales as double),\n","    Holiday_Flag,\n","    cast(Temperature as float),\n","    cast(fuel_price as float),\n","    cast(cpi as double),\n","    cast(Unemployment as float)\n","from default.raw_sales;\n","</code>\n","- Queries to answer questions\n"," - Which store has minimum and maximum sales?\n","<code>\n","    with overall_sales as\n","(select Store,sum(Weekly_Sales) as total from walmart_sales group by Store)\n","select store,total,CAST(total AS DECIMAL(38,10)),ROUND(CAST(total AS DECIMAL(38,10)),2),dense_rank() over( order by total desc) from overall_sales;\n","    </code>\n"," - Which store has maximum standard deviation?<code>\n","select * from (select store,stddev(weekly_sales) as a from walmart_sales group by store) a order by a desc ;</code>\n"," - Coefficient of mean to standard deviation?<code>\n"," select store,stddev(weekly_sales)/avg(weekly_sales) from walmart_sales group by store;\n","</code>\n"," - Which store/s has good quarterly growth rate in Q3’2012?<code>\n","with Q3 as\n","(select store,sum(Weekly_Sales) as Quarter_Sales,'Q3' as Quarter from walmart_sales where date_of_entry &gt; '2012-07-01' and date_of_entry &lt; '2012-09-30' group by store)\n",",Q2 as\n","(select store,sum(Weekly_Sales) as Quarter_Sales,'Q2' as Quarter from walmart_sales where date_of_entry &gt; '2012-04-01' and date_of_entry &lt; '2012-06-30' group by store)\n",",Q2_Q3 as\n","(select * from Q3 union select * from Q2)\n",",final as\n","(select store,Quarter,(Quarter_Sales - lag(Quarter_Sales) over (partition by store order by Quarter)) as difference,round(100 * (Quarter_Sales - lag(Quarter_Sales) over (partition by store order by Quarter))/ lag(Quarter_Sales) over (partition by store order by Quarter),2) as growth_rate from Q2_Q3)\n","select * from final where growth_rate > 0 order by growth_rate desc;\n","    </code>\n"," - Find out holidays which have higher sales than the mean sales in non-holiday season for all stores together.<code>\n","    set non_holiday_mean = select avg(weekly_sales) from walmart_sales;\n"," set hive.strict.checks.cartesian.product=false;\n","select h.h_sales,h.date_of_entry from (select avg(Weekly_Sales) as h_sales,date_of_entry from walmart_sales where Holiday_Flag=1 group by date_of_entry) as h\n","left join (select avg(Weekly_Sales) as nh_avg from walmart_sales where Holiday_Flag=0) as nh\n","where h.h_sales > nh.nh_avg;\n","</code>\n","\n","## <font color=blue>Project Overview</font>"],"id":"86ec60fe"}]}